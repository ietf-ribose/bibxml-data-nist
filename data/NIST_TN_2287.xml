<reference anchor="NIST.TN.2287" target="https://doi.org/10.6028/NIST.TN.2287">
  <front>
    <title>Human-in-the-loop technical document annotation</title>
    <author fullname="Fung, Juan F.">
      <organization/>
    </author>
    <author fullname="Li, Zongxia.">
      <organization/>
    </author>
    <author fullname="Stephens, Daniel Kofi.">
      <organization/>
    </author>
    <author fullname="Mao, Andrew.">
      <organization/>
    </author>
    <author fullname="Goel, Pranav.">
      <organization/>
    </author>
    <author fullname="Walpole, Emily.">
      <organization/>
    </author>
    <author fullname="Dima, Alden.">
      <organization/>
    </author>
    <author fullname="Boyd-Graber, Jordan Lee.">
      <organization/>
    </author>
    <author>
      <organization>National Institute of Standards and Technology (U.S.)</organization>
    </author>
    <abstract>In this report, we address the following question: to what extent can machine learning assist a human with traditional text analysis, such as content analysis or grounded theory in the social sciences? In practice, such tasks require humans to review and categorize(e.g., by manually annotating the text with labels) a large sample of documents. We do not expect nor necessarily desire the machine to automate the tasks the human would otherwise perform, but rather want to find ways to help the human to perform the tasks more efficiently. We present a modular implementation of a system that incorporates supervised (active learning) and unsupervised (topic modeling) methods to assist humans with technical document annotation. The implemented system allows us to conduct user studies to evaluate the usefulness of machine assistance. We present results from two such user studies and highlight directions for future research.</abstract>
  </front>
  <seriesInfo name="DOI" value="10.6028/NIST.TN.2287"/>
  <seriesInfo name="NIST technical note; NIST tech note; NIST TN" value="2287"/>
</reference>