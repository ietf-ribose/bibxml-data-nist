<reference anchor="NIST.SP.500-326" target="https://doi.org/10.6028/NIST.SP.500-326">
  <front>
    <title>SATE V report</title>
    <author fullname="Delaitre, AurÃ©lien.">
      <organization/>
    </author>
    <author fullname="Stivalet, Bertrand.">
      <organization/>
    </author>
    <author fullname="Black, Paul E.">
      <organization/>
    </author>
    <author fullname="Okun, Vadim.">
      <organization/>
    </author>
    <author fullname="Ribeiro, Athos.">
      <organization/>
    </author>
    <author fullname="Cohen, Terry S.">
      <organization/>
    </author>
    <author>
      <organization>Information Technology Laboratory (National Institute of Standards and Technology)</organization>
    </author>
    <abstract>Software assurance has been the focus of the National Institute of Standards and Technology (NIST) Software Assurance Metrics and Tool Evaluation (SAMATE) team for many years. The Static Analysis Tool Exposition (SATE) is one of the team s prominent projects to advance research in and adoption of static analysis, one of several software assurance methods. This report describes our approach and methodology. It then presents and discusses the results collected from the fifth edition of SATE. Overall, the goal of SATE was not to rank static analysis tools, but rather to propose a methodology to assess tool effectiveness. Others can use this methodology to determine which tools fit their requirements. The results in this report are presented as examples and used as a basis for further discussion. Our methodology relies on metrics, such as recall and precision, to determine tool effectiveness. To calculate these metrics, we designed test cases that exhibit certain characteristics. Most of the test cases were large pieces of software with cybersecurity implications. Fourteen participants ran their tools on these test cases and sent us a report of their findings. We analyzed these reports and calculated the metrics to assess the tools effectiveness. Although a few results remained inconclusive, many key elements could be inferred based on our methodology, test cases, and analysis. In particular, we were able to estimate the propensity of tools to find critical vulnerabilities in real software, the degree of noise they produced, and the type of weaknesses they were able to find. Some shortcomings in the methodology and test cases were also identified and solutions proposed for the next edition of SATE.</abstract>
  </front>
  <seriesInfo name="DOI" value="NIST.SP.500-326"/>
  <seriesInfo name="NIST special publication; NIST special pub; NIST SP" value="500-326"/>
</reference>