<reference anchor="NIST.IR.8383" target="https://doi.org/10.6028/NIST.IR.8383">
  <front>
    <title>Standard errors and significance testing in data analysis for testing classifiers</title>
    <author fullname="Wu, Jin Chu.">
      <organization/>
    </author>
    <author fullname="Kacker, Raghu N.">
      <organization/>
    </author>
    <author>
      <organization>National Institute of Standards and Technology (U.S.)</organization>
    </author>
    <abstract>The one-classifier and two-classifier significance testing for evaluation and comparison of classifiers are conducted to investigate the statistical significance of differences and provide quantitative information in terms of the significance level, i.e., p-value, in a new ROC analysis where three score distributions and two decision thresholds are employed, and data dependency caused by multiple use of the same subjects is involved. To analyze the performance of classifiers, the standard error of the cost function is estimated using the nonparametric three-sample two-layer bootstrap algorithm on a two-layer data structure constructed after dataset optimization, based on our prior rigorous statistical research in ROC analysis on large datasets with data dependency. In comparison, the positive correlation coefficient must be taken into consideration, which is computed using a synchronized resampling algorithm; otherwise, the likelihood of detecting the statistical significance of difference between the performance levels of two classifiers can be wrongly reduced.</abstract>
  </front>
  <seriesInfo name="DOI" value="NIST.IR.8383"/>
  <seriesInfo name="NISTIR; NIST IR; NIST interagency report; NIST internal report" value="8383"/>
</reference>